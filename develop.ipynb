{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tutte le dipendenze sono installate correttamente\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Aggiungi il path del progetto\n",
    "project_root = Path.cwd()\n",
    "src_path = project_root / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "# Test importazione dipendenze\n",
    "try:\n",
    "    import asyncpg\n",
    "    import aiosql\n",
    "    import logfire\n",
    "    from dotenv import load_dotenv\n",
    "    print(\"✅ Tutte le dipendenze sono installate correttamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Manca la dipendenza: {e.name}\")\n",
    "    print(f\"Installa con: uv pip install {e.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory SQL: /home/sam/github/scrapy-playwright-scrapegraphai/src/crawler/sql\n",
      "Esiste: True\n",
      "✅ Directory SQL creata/verificata\n"
     ]
    }
   ],
   "source": [
    "# Verifica struttura directory SQL\n",
    "sql_dir = project_root / 'src' / 'crawler' / 'sql'\n",
    "print(f\"Directory SQL: {sql_dir}\")\n",
    "print(f\"Esiste: {sql_dir.exists()}\")\n",
    "\n",
    "# Crea directory SQL se non esiste\n",
    "sql_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"✅ Directory SQL creata/verificata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connessione al database riuscita\n",
      "Versione PostgreSQL: PostgreSQL 16.4 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.2.0, 64-bit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test connessione database\n",
    "load_dotenv()  # Carica variabili ambiente dal file .env\n",
    "\n",
    "async def test_db_connection():\n",
    "    conn_params = {\n",
    "        'host': os.getenv('POSTGRES_HOST', 'localhost'),\n",
    "        'port': int(os.getenv('POSTGRES_PORT', 5432)),\n",
    "        'user': os.getenv('POSTGRES_USER', 'postgres'),\n",
    "        'password': os.getenv('POSTGRES_PASSWORD', ''),\n",
    "        'database': os.getenv('POSTGRES_DATABASE', 'postgres')\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        conn = await asyncpg.connect(**conn_params)\n",
    "        version = await conn.fetchval('SELECT version()')\n",
    "        print(f\"✅ Connessione al database riuscita\\nVersione PostgreSQL: {version}\")\n",
    "        await conn.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore di connessione: {e}\")\n",
    "        return False\n",
    "\n",
    "# Esegui test connessione\n",
    "await test_db_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File SQL creati/aggiornati\n"
     ]
    }
   ],
   "source": [
    "# Modifica della cella 4 con il nuovo schema\n",
    "schema_sql = \"\"\"\n",
    "-- name: create_schema!\n",
    "CREATE TABLE IF NOT EXISTS frontier_url (\n",
    "    id BIGSERIAL PRIMARY KEY,\n",
    "    url TEXT NOT NULL,\n",
    "    category TEXT NOT NULL,\n",
    "    type INTEGER NOT NULL CHECK (type >= 0 AND type <= 2),\n",
    "    depth INTEGER NOT NULL DEFAULT 0,\n",
    "    is_target BOOLEAN NOT NULL DEFAULT false,\n",
    "    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n",
    "    processed_at TIMESTAMP WITH TIME ZONE,\n",
    "    failed_attempts INTEGER DEFAULT 0,\n",
    "    last_error TEXT,\n",
    "    UNIQUE(url, category)\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_frontier_url_status \n",
    "    ON frontier_url(processed_at) \n",
    "    WHERE processed_at IS NULL;\n",
    "CREATE INDEX IF NOT EXISTS idx_frontier_url_category \n",
    "    ON frontier_url(category);\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS config_url_log (\n",
    "    id BIGSERIAL PRIMARY KEY,\n",
    "    url TEXT NOT NULL,\n",
    "    category TEXT NOT NULL,\n",
    "    type INTEGER NOT NULL CHECK (type >= 0 AND type <= 2),\n",
    "    last_checked_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n",
    "    success_count INTEGER DEFAULT 0,\n",
    "    failure_count INTEGER DEFAULT 0,\n",
    "    last_status TEXT,\n",
    "    UNIQUE(url, category)\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_config_url_log_category \n",
    "    ON config_url_log(category);\n",
    "\"\"\"\n",
    "\n",
    "queries_sql = \"\"\"\n",
    "-- name: insert_frontier_url<!\n",
    "INSERT INTO frontier_url (url, category, type, depth, is_target)\n",
    "VALUES (:url, :category, :type_, :depth, :is_target)\n",
    "ON CONFLICT (url, category) DO NOTHING\n",
    "RETURNING id;\n",
    "\n",
    "-- name: get_unprocessed_urls\n",
    "SELECT id, url, category, type, depth, is_target \n",
    "FROM frontier_url \n",
    "WHERE processed_at IS NULL \n",
    "ORDER BY created_at ASC \n",
    "LIMIT :limit;\n",
    "\n",
    "-- name: mark_url_processed!\n",
    "UPDATE frontier_url \n",
    "SET processed_at = CURRENT_TIMESTAMP,\n",
    "    failed_attempts = CASE WHEN :success THEN failed_attempts \n",
    "                         ELSE failed_attempts + 1 END,\n",
    "    last_error = CASE WHEN :success THEN NULL ELSE :error_message END\n",
    "WHERE id = :url_id;\n",
    "\n",
    "-- name: update_config_url_log!\n",
    "INSERT INTO config_url_log (\n",
    "    url, category, type, last_checked_at, \n",
    "    success_count, failure_count, last_status\n",
    ")\n",
    "VALUES (\n",
    "    :url, :category, :type_, CURRENT_TIMESTAMP, \n",
    "    CASE WHEN :success THEN 1 ELSE 0 END,\n",
    "    CASE WHEN :success THEN 0 ELSE 1 END,\n",
    "    :status\n",
    ")\n",
    "ON CONFLICT (url, category) \n",
    "DO UPDATE SET\n",
    "    last_checked_at = CURRENT_TIMESTAMP,\n",
    "    success_count = CASE WHEN :success \n",
    "                       THEN config_url_log.success_count + 1 \n",
    "                       ELSE config_url_log.success_count END,\n",
    "    failure_count = CASE WHEN :success \n",
    "                       THEN config_url_log.failure_count \n",
    "                       ELSE config_url_log.failure_count + 1 END,\n",
    "    last_status = :status;\n",
    "\"\"\"\n",
    "\n",
    "# Scrivi i file SQL\n",
    "with open(sql_dir / 'schema.sql', 'w') as f:\n",
    "    f.write(schema_sql)\n",
    "\n",
    "with open(sql_dir / 'queries.sql', 'w') as f:\n",
    "    f.write(queries_sql)\n",
    "\n",
    "print(\"✅ File SQL creati/aggiornati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/github/scrapy-playwright-scrapegraphai/src/crawler/database.py:28: LogfireNotConfiguredWarning: No logs or spans will be created until `logfire.configure()` has been called. Set the environment variable LOGFIRE_IGNORE_NO_CONFIG=1 or add ignore_no_config=true in pyproject.toml to suppress this warning.\n",
      "  logfire.info(\"SQL queries loaded successfully\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ URL stored with ID: 1\n",
      "✅ Retrieved 1 unprocessed URLs\n",
      "✅ URL marked as processed\n",
      "✅ Database pool closed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test DatabaseManager\n",
    "from crawler.database import DatabaseManager\n",
    "\n",
    "async def test_database_manager():\n",
    "    db = DatabaseManager()\n",
    "    try:\n",
    "        # Inizializza il database\n",
    "        await db.initialize()\n",
    "        \n",
    "        # Test inserimento URL\n",
    "        test_url = \"https://webapps.unito.it/concorsiweb/visualizzaperweb.php?tipo=25&p=y&C7=all&criterio35=A&criterio46=No\"\n",
    "        test_category = \"Torino\"\n",
    "        \n",
    "        # Store URL\n",
    "        url_id = await db.store_frontier_url(\n",
    "            url=test_url,\n",
    "            category=test_category,\n",
    "            type_=1\n",
    "        )\n",
    "        print(f\"✅ URL stored with ID: {url_id}\")\n",
    "        \n",
    "        # Get unprocessed URLs\n",
    "        urls = await db.get_unprocessed_frontier_urls(1)\n",
    "        print(f\"✅ Retrieved {len(urls)} unprocessed URLs\")\n",
    "        \n",
    "        # Mark as processed\n",
    "        if urls:\n",
    "            await db.mark_url_processed(urls[0]['id'])\n",
    "            print(\"✅ URL marked as processed\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if hasattr(db, 'pool') and db.pool:\n",
    "            await db.pool.close()\n",
    "            print(\"✅ Database pool closed\")\n",
    "\n",
    "# Esegui test DatabaseManager\n",
    "await test_database_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop.ipynb\n",
    "\n",
    "# %% [markdown]\n",
    "# # Crawler Development and Testing\n",
    "# Let's test each component step by step\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Environment Setup and Imports\n",
    "\n",
    "# %%\n",
    "import asyncio\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import logfire\n",
    "from crawler.database import db_manager\n",
    "from crawler.utils.logging_utils import setup_logging\n",
    "from crawler.utils.playwright_utils import PlaywrightPageManager\n",
    "from crawler.items import UrlItem, ConfigUrlLogItem\n",
    "\n",
    "# Setup logging\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories found: ['Bologna', 'Torino']\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Config Loading and Validation\n",
    "\n",
    "# %%\n",
    "def load_config(config_path: str = \"config/crawler_config.yaml\") -> dict:\n",
    "    with open(config_path) as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Test config loading\n",
    "config = load_config()\n",
    "print(\"Categories found:\", [cat[\"name\"] for cat in config[\"categories\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type 0 URLs: 0\n",
      "\n",
      "Type 1 URLs: 2\n",
      "- https://webapps.unito.it/concorsiweb/visualizzaperweb.php?tipo=25&p=y&C7=all&criterio35=A&criterio46=No (Torino)\n",
      "- https://webapps.unito.it/concorsiweb/visualizzaperweb.php?tipo=24&p=y&C7=all&criterio35=A&criterio46=No (Torino)\n",
      "\n",
      "Type 2 URLs: 3\n",
      "- https://bandi.unibo.it/agevolazioni/premi-laurea (Bologna)\n",
      "- https://bandi.unibo.it/agevolazioni/borse?b_start:int=20 (Bologna)\n",
      "- https://bandi.unibo.it/agevolazioni/borse (Bologna)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. URL Extraction by Type\n",
    "\n",
    "# %%\n",
    "def extract_urls_by_type(config: dict, type_: int) -> list:\n",
    "    \"\"\"Extract URLs of specific type from config\"\"\"\n",
    "    urls = []\n",
    "    for category in config[\"categories\"]:\n",
    "        category_name = category[\"name\"]\n",
    "        for url_config in category[\"urls\"]:\n",
    "            if url_config[\"type\"] == type_:\n",
    "                urls.append({\n",
    "                    \"url\": url_config[\"url\"],\n",
    "                    \"category\": category_name,\n",
    "                    \"type\": url_config[\"type\"],\n",
    "                    \"target_patterns\": url_config[\"target_patterns\"],\n",
    "                    \"seed_pattern\": url_config.get(\"seed_pattern\"),\n",
    "                    \"max_depth\": url_config.get(\"max_depth\", 0)\n",
    "                })\n",
    "    return urls\n",
    "\n",
    "# Test URL extraction for each type\n",
    "for type_ in range(3):\n",
    "    urls = extract_urls_by_type(config, type_)\n",
    "    print(f\"\\nType {type_} URLs: {len(urls)}\")\n",
    "    for url in urls:\n",
    "        print(f\"- {url['url']} ({url['category']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Database Connection Test\n",
    "\n",
    "# %%\n",
    "async def test_database():\n",
    "    \"\"\"Test database connection and basic operations\"\"\"\n",
    "    try:\n",
    "        await db_manager.initialize()\n",
    "        print(\"Database initialized successfully\")\n",
    "        \n",
    "        # Test storing a URL\n",
    "        test_url = \"https://test.com/example\"\n",
    "        url_id = await db_manager.store_frontier_url(\n",
    "            url=test_url,\n",
    "            category=\"test\",\n",
    "            type_=1,\n",
    "            depth=0,\n",
    "            is_target=True\n",
    "        )\n",
    "        print(f\"Stored test URL with ID: {url_id}\")\n",
    "        \n",
    "        # Test config URL log\n",
    "        await db_manager.update_config_url_log(\n",
    "            url=test_url,\n",
    "            category=\"test\",\n",
    "            type_=1,\n",
    "            success=True,\n",
    "            status=\"test_completed\"\n",
    "        )\n",
    "        print(\"Updated config URL log successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Database test failed: {e}\")\n",
    "    finally:\n",
    "        await db_manager.close()\n",
    "        print(\"Database connection closed\")\n",
    "\n",
    "# Run database test\n",
    "await test_database()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Playwright Utils Test\n",
    "\n",
    "# %%\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async def test_playwright_utils(test_url: str = \"https://example.com\"):\n",
    "    \"\"\"Test PlaywrightPageManager functionality\"\"\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        \n",
    "        # Initialize page manager\n",
    "        manager = PlaywrightPageManager(page)\n",
    "        \n",
    "        try:\n",
    "            # Test basic navigation\n",
    "            await page.goto(test_url)\n",
    "            await manager.initialize_page()\n",
    "            \n",
    "            # Test link extraction\n",
    "            links = await manager.extract_links()\n",
    "            print(f\"Found {len(links)} links on {test_url}\")\n",
    "            \n",
    "            # Test pattern-based extraction\n",
    "            pdf_links = await manager.extract_links(r\".*\\.pdf$\")\n",
    "            print(f\"Found {len(pdf_links)} PDF links\")\n",
    "            \n",
    "        finally:\n",
    "            await manager.cleanup()\n",
    "            await browser.close()\n",
    "\n",
    "# Run Playwright test\n",
    "await test_playwright_utils()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Test Type 0 Processing\n",
    "\n",
    "# %%\n",
    "async def test_type_0_processing():\n",
    "    \"\"\"Test processing of Type 0 URLs\"\"\"\n",
    "    # Get Type 0 URLs from config\n",
    "    type_0_urls = extract_urls_by_type(config, 0)\n",
    "    \n",
    "    try:\n",
    "        await db_manager.initialize()\n",
    "        \n",
    "        for url_config in type_0_urls:\n",
    "            # Create URL item\n",
    "            item = UrlItem(\n",
    "                url=url_config[\"url\"],\n",
    "                category=url_config[\"category\"],\n",
    "                type=0,\n",
    "                depth=0,\n",
    "                is_target=True,\n",
    "                found_on=None\n",
    "            )\n",
    "            \n",
    "            # Store in database\n",
    "            url_id = await db_manager.store_frontier_url(\n",
    "                url=item[\"url\"],\n",
    "                category=item[\"category\"],\n",
    "                type_=item[\"type\"],\n",
    "                depth=item[\"depth\"],\n",
    "                is_target=item[\"is_target\"]\n",
    "            )\n",
    "            \n",
    "            print(f\"Processed Type 0 URL: {item['url']}\")\n",
    "            \n",
    "    finally:\n",
    "        await db_manager.close()\n",
    "\n",
    "# Run Type 0 test\n",
    "await test_type_0_processing()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Test Type 1 Processing\n",
    "\n",
    "# %%\n",
    "async def test_type_1_processing():\n",
    "    \"\"\"Test processing of a single Type 1 URL\"\"\"\n",
    "    # Get first Type 1 URL from config\n",
    "    type_1_urls = extract_urls_by_type(config, 1)\n",
    "    if not type_1_urls:\n",
    "        print(\"No Type 1 URLs found in config\")\n",
    "        return\n",
    "    \n",
    "    test_config = type_1_urls[0]\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        manager = PlaywrightPageManager(page)\n",
    "        \n",
    "        try:\n",
    "            await db_manager.initialize()\n",
    "            \n",
    "            # Navigate to URL\n",
    "            await page.goto(test_config[\"url\"])\n",
    "            await manager.initialize_page()\n",
    "            \n",
    "            # Extract target URLs\n",
    "            target_urls = set()\n",
    "            for pattern in test_config[\"target_patterns\"]:\n",
    "                urls = await manager.extract_links(pattern)\n",
    "                target_urls.update(urls)\n",
    "            \n",
    "            print(f\"Found {len(target_urls)} target URLs\")\n",
    "            \n",
    "            # Store first 3 URLs as example\n",
    "            for url in list(target_urls)[:3]:\n",
    "                item = UrlItem(\n",
    "                    url=url,\n",
    "                    category=test_config[\"category\"],\n",
    "                    type=1,\n",
    "                    depth=0,\n",
    "                    is_target=True,\n",
    "                    found_on=test_config[\"url\"]\n",
    "                )\n",
    "                \n",
    "                await db_manager.store_frontier_url(\n",
    "                    url=item[\"url\"],\n",
    "                    category=item[\"category\"],\n",
    "                    type_=item[\"type\"],\n",
    "                    depth=item[\"depth\"],\n",
    "                    is_target=item[\"is_target\"]\n",
    "                )\n",
    "                \n",
    "                print(f\"Stored target URL: {url}\")\n",
    "                \n",
    "        finally:\n",
    "            await manager.cleanup()\n",
    "            await browser.close()\n",
    "            await db_manager.close()\n",
    "\n",
    "# Run Type 1 test\n",
    "await test_type_1_processing()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Full Spider Test\n",
    "\n",
    "# %%\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from crawler.spiders.frontier_spider import FrontierSpider\n",
    "\n",
    "def test_spider():\n",
    "    \"\"\"Run the full spider\"\"\"\n",
    "    process = CrawlerProcess(get_project_settings())\n",
    "    process.crawl(FrontierSpider)\n",
    "    process.start()\n",
    "\n",
    "# Uncomment to run full spider test\n",
    "# test_spider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
